{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1 \u2014 Guided Assignment (Human Factors, Privacy & Security)\n",
        "\n",
        "**Goal:** Build your first mini dataset from public privacy/security-related pages.\n",
        "\n",
        "**Deliverable:** A list of dictionaries named `dataset` with at least **5 rows**.\n",
        "Each row should include:\n",
        "- `url`\n",
        "- `status`\n",
        "- `title`\n",
        "- `num_links`\n",
        "- `mentions_cookies`\n",
        "- `mentions_privacy`\n",
        "- `mentions_security`\n",
        "\n",
        "**Time target:** ~60\u201390 minutes\n",
        "\n",
        "---\n",
        "\n",
        "## Step 0 \u2014 Imports\n",
        "Run the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 \u2014 Choose your 5 URLs (public pages)\n",
        "\n",
        "Pick pages relevant to human factors of privacy & security, for example:\n",
        "- privacy policy pages\n",
        "- account security help pages\n",
        "- cookie policy pages\n",
        "- \u201chow we use your data\u201d pages\n",
        "- NIST / ISO / ENISA public pages\n",
        "- university lab pages that mention privacy/security\n",
        "\n",
        "Add them to the list below.\n",
        "\n",
        "Tip: mix sources so your dataset is interesting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = [\n",
        "    # Replace these with your chosen pages (at least 5)\n",
        "    \"https://www.mozilla.org/en-US/privacy/\",\n",
        "    \"https://support.google.com/accounts/answer/6294825?hl=en\",  # account security example\n",
        "    \"https://www.wikipedia.org/\",\n",
        "    \"https://www.nist.gov/privacy-framework\",\n",
        "    \"https://www.enisa.europa.eu/topics/data-protection\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 \u2014 Implement `analyze_page(url)`\n",
        "\n",
        "Use the template below and fill in the TODOs.\n",
        "\n",
        "Hints:\n",
        "- `requests.get(url, timeout=20)`\n",
        "- `BeautifulSoup(r.text, \"html.parser\")`\n",
        "- `soup.find(\"title\")`\n",
        "- `soup.find_all(\"a\")`\n",
        "- `soup.get_text(\" \", strip=True).lower()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_page(url: str, timeout: int = 20) -> dict:\n",
        "    # TODO 1: fetch the page\n",
        "    r = requests.get(url, timeout=timeout)\n",
        "\n",
        "    # TODO 2: parse HTML\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "    # TODO 3: title extraction (handle missing title)\n",
        "    title_tag = soup.find(\"title\")\n",
        "    title = title_tag.get_text(strip=True) if title_tag else None\n",
        "\n",
        "    # TODO 4: count links\n",
        "    links = soup.find_all(\"a\")\n",
        "\n",
        "    # TODO 5: build a lowercased text blob for keyword checks\n",
        "    text_lower = soup.get_text(\" \", strip=True).lower()\n",
        "\n",
        "    # TODO 6: return a dict row (schema below)\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"status\": r.status_code,\n",
        "        \"title\": title,\n",
        "        \"num_links\": len(links),\n",
        "        \"mentions_cookies\": (\"cookie\" in text_lower),\n",
        "        \"mentions_privacy\": (\"privacy\" in text_lower),\n",
        "        \"mentions_security\": (\"security\" in text_lower),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 \u2014 Build the dataset (list of dicts)\n",
        "\n",
        "Run the function across your URLs, handling errors.\n",
        "\n",
        "Expected:\n",
        "- `dataset` ends up with one dict per URL\n",
        "- If a URL fails, store `{\"url\": ..., \"error\": ...}`\n",
        "\n",
        "Fill the TODO.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = []\n",
        "\n",
        "for u in urls:\n",
        "    try:\n",
        "        row = analyze_page(u)\n",
        "        dataset.append(row)\n",
        "    except Exception as e:\n",
        "        dataset.append({\"url\": u, \"error\": str(e)})\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 \u2014 Quick checks\n",
        "\n",
        "You should confirm:\n",
        "- How many rows succeeded?\n",
        "- Which pages mention \u201cprivacy\u201d?\n",
        "- Which pages mention \u201ccookies\u201d?\n",
        "\n",
        "Fill in the TODOs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: how many rows have an 'error' key?\n",
        "num_errors = sum(1 for row in dataset if \"error\" in row)\n",
        "\n",
        "# TODO: count mentions of privacy/cookies/security (ignore errored rows)\n",
        "privacy_count = sum(1 for row in dataset if row.get(\"mentions_privacy\") is True)\n",
        "cookies_count = sum(1 for row in dataset if row.get(\"mentions_cookies\") is True)\n",
        "security_count = sum(1 for row in dataset if row.get(\"mentions_security\") is True)\n",
        "\n",
        "print(\"Total rows:\", len(dataset))\n",
        "print(\"Errors:\", num_errors)\n",
        "print(\"Mentions privacy:\", privacy_count)\n",
        "print(\"Mentions cookies:\", cookies_count)\n",
        "print(\"Mentions security:\", security_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 \u2014 Reflection (research lens)\n",
        "\n",
        "Write 3\u20135 bullet points answering:\n",
        "- Which pages were easiest/hardest to scrape and why?\n",
        "- Does keyword presence (\u201ccookie\u201d, \u201cprivacy\u201d) capture what you care about in human factors research?\n",
        "- What signals might be more meaningful (e.g., readability, presence of opt-out instructions, headings like \u201cYour choices\u201d)?\n",
        "\n",
        "(Just write below in Markdown.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### My reflection\n",
        "\n",
        "- \n",
        "- \n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional stretch (if you finish early)\n",
        "\n",
        "Add one more field to each row:\n",
        "- `num_words`: count words in the page text\n",
        "\n",
        "Hint:\n",
        "```python\n",
        "words = soup.get_text(\" \", strip=True).split()\n",
        "num_words = len(words)\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "title": "Week 1 Assignment \u2014 Guided Scraping (Privacy & Security)"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}