{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1 ‚Äî Python for Research Data Collection (Human Factors, Privacy & Security)\n",
        "\n",
        "**Time budget:** ~2 hours  \n",
        "**Goal:** Get comfortable using Python (in a notebook) to fetch a web page, extract a few signals, and store them in simple data structures.\n",
        "\n",
        "This week is intentionally ‚Äúsmall but real.‚Äù We‚Äôll scrape **public pages** that are relevant to privacy & security research (e.g., *privacy policy* pages).\n",
        "\n",
        "---\n",
        "\n",
        "## What you‚Äôll learn\n",
        "### Python fundamentals (applied)\n",
        "- Variables and basic data types (`str`, `int`, `float`, `bool`, `None`)\n",
        "- Lists and dictionaries (your first ‚Äúdataset‚Äù)\n",
        "- `for` loops and `if` statements\n",
        "- Writing small functions\n",
        "\n",
        "### Web fundamentals (lightweight intro)\n",
        "- What HTTP is (request ‚Üí response)\n",
        "- HTML as a tree (tags)\n",
        "- How a ‚Äúscraper‚Äù works at the simplest level\n",
        "\n",
        "### Output\n",
        "By the end you will produce a **list of dictionaries** like:\n",
        "\n",
        "```python\n",
        "[\n",
        "  {\"url\": \"...\", \"title\": \"...\", \"num_links\": 42, \"mentions_cookies\": True},\n",
        "  ...\n",
        "]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "We‚Äôll use:\n",
        "- `requests` to download HTML\n",
        "- `bs4` (BeautifulSoup) to parse HTML\n",
        "\n",
        "If you run this notebook locally and don‚Äôt have these installed, run:\n",
        "\n",
        "```bash\n",
        "pip install requests beautifulsoup4\n",
        "```\n",
        "\n",
        "In many notebook environments, they may already be available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Variables + data types (in the context of scraping)\n",
        "\n",
        "When we scrape, we deal with:\n",
        "- URLs (strings)\n",
        "- status codes (integers)\n",
        "- HTML text (strings)\n",
        "- flags like ‚Äúmentions cookies‚Äù (booleans)\n",
        "- ‚Äúmaybe missing‚Äù values (`None`)\n",
        "\n",
        "Let‚Äôs create a few on purpose.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://www.wikipedia.org/\"\n",
        "status_code_example = 200\n",
        "mentions_cookies = False\n",
        "missing_value = None\n",
        "\n",
        "print(type(url), url)\n",
        "print(type(status_code_example), status_code_example)\n",
        "print(type(mentions_cookies), mentions_cookies)\n",
        "print(type(missing_value), missing_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ca45fa",
      "metadata": {},
      "source": [
        "### üß† Concept: The Client-Server Model (HTTP)\n",
        "\n",
        "Imagine ordering at a restaurant:\n",
        "1.  **You (Client)**: \"I'd like a burger, please.\" (This is the **Request**)\n",
        "2.  **Waiter (Server)**: Checks kitchen, comes back with a burger on a plate. (This is the **Response**)\n",
        "\n",
        "In the web world:\n",
        "-   **You** are `requests` (Python).\n",
        "-   **The Restaurant** is Wikipedia.\n",
        "-   The **Burger** is the HTML code.\n",
        "-   The **Plate** is the Response object (holding the HTML, the status code, etc)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8f704a",
      "metadata": {},
      "source": [
        "## 2) Your first HTTP request\n",
        "\n",
        "`requests.get(url)` sends an HTTP GET request.\n",
        "\n",
        "Useful fields on the response:\n",
        "- `response.status_code` (e.g., 200 = OK)\n",
        "- `response.text` (HTML as a string)\n",
        "- `response.headers` (metadata)\n",
        "\n",
        "We‚Äôll fetch one page and inspect it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9a98364",
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://www.wikipedia.org/\"\n",
        "response = requests.get(url, timeout=20)\n",
        "\n",
        "print(\"Status:\", response.status_code)\n",
        "print(\"Content-Type:\", response.headers.get(\"Content-Type\"))\n",
        "print(\"First 300 characters of HTML:\")\n",
        "print(response.text[:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abd6d6f0",
      "metadata": {},
      "source": [
        "### üß† Concept: What is HTML?\n",
        "\n",
        "HTML is just **text with tags**.\n",
        "\n",
        "Think of it like a Russian Matryoshka doll or a family tree:\n",
        "-   `<html>` is the grandmother.\n",
        "-   `<body>` is the mother.\n",
        "-   `<div>` are the children.\n",
        "-   `<p>` (paragraphs) and `<a>` (links) are the grandchildren.\n",
        "\n",
        "**Tags** tell the browser (and us) what the content *is*.\n",
        "-   `<p>` = Paragraph\n",
        "-   `<a>` = Anchor (Link)\n",
        "-   `<h1>` = Header 1 (Big Title)\n",
        "\n",
        "**Attributes** give extra info:\n",
        "-   `<a href=\"https://google.com\">` -> `href` tells us *where* the link goes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) HTML parsing: turning text into a tree\n",
        "\n",
        "HTML is a nested structure. BeautifulSoup helps you query it.\n",
        "\n",
        "Key ideas:\n",
        "- Tags like `<title>`, `<a>`, `<p>`\n",
        "- Attributes like `href`, `class`, `id`\n",
        "- `.find(...)` gets one element\n",
        "- `.find_all(...)` gets many elements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "title_tag = soup.find(\"title\")\n",
        "print(\"Title tag:\", title_tag)\n",
        "print(\"Title text:\", title_tag.get_text(strip=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Lists: collecting repeated items (links)\n",
        "\n",
        "A typical scraping pattern:\n",
        "1. Find many elements (`find_all`)\n",
        "2. Loop through them\n",
        "3. Extract what you want\n",
        "4. Store results in a list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "links = soup.find_all(\"a\")\n",
        "print(\"Number of <a> tags:\", len(links))\n",
        "\n",
        "# Take a quick look at the first 5 links\n",
        "for a in links[:5]:\n",
        "    print(\"-\", a.get_text(strip=True)[:40], \"=>\", a.get(\"href\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Dictionaries: making each row of data structured\n",
        "\n",
        "For research work, a dictionary is a great ‚Äúrow‚Äù format:\n",
        "- consistent keys\n",
        "- readable\n",
        "- easy to convert later to CSV/JSON\n",
        "\n",
        "Let‚Äôs create a small record for a page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "record = {\n",
        "    \"url\": url,\n",
        "    \"title\": title_tag.get_text(strip=True),\n",
        "    \"num_links\": len(links),\n",
        "}\n",
        "record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) `if` statements: extracting a simple privacy-related signal\n",
        "\n",
        "Human factors privacy & security research often analyzes:\n",
        "- language like ‚Äúcookies‚Äù, ‚Äúthird-party‚Äù, ‚Äúconsent‚Äù, ‚Äúdata retention‚Äù\n",
        "- user choices: opt-out, control, settings\n",
        "- transparency cues: ‚Äúwe collect‚Ä¶‚Äù, ‚Äúwe share‚Ä¶‚Äù\n",
        "\n",
        "We‚Äôll implement a simple detector: does a page mention **cookies**?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "html_lower = response.text.lower()\n",
        "mentions_cookies = \"cookie\" in html_lower  # crude but useful start\n",
        "mentions_cookies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Functions: turning steps into a reusable tool\n",
        "\n",
        "A function packages logic so you can apply it to *many* pages.\n",
        "\n",
        "We‚Äôll build `analyze_page(url)` that returns one dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_page(url: str, timeout: int = 20) -> dict:\n",
        "    \"\"\"Fetch a URL and extract a few simple signals.\n",
        "\n",
        "    Returns a dict suitable for putting into a list (dataset).\n",
        "    \"\"\"\n",
        "    r = requests.get(url, timeout=timeout)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    \n",
        "    title = None\n",
        "    title_tag = soup.find(\"title\")\n",
        "    if title_tag:\n",
        "        title = title_tag.get_text(strip=True)\n",
        "\n",
        "    links = soup.find_all(\"a\")\n",
        "    text_lower = soup.get_text(\" \", strip=True).lower()\n",
        "\n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"status\": r.status_code,\n",
        "        \"title\": title,\n",
        "        \"num_links\": len(links),\n",
        "        \"mentions_cookies\": (\"cookie\" in text_lower),\n",
        "        \"mentions_privacy\": (\"privacy\" in text_lower),\n",
        "        \"mentions_security\": (\"security\" in text_lower),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyze_page(\"https://www.wikipedia.org/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Mini ‚Äúdataset‚Äù: list of dictionaries\n",
        "\n",
        "Now we can run the same function across multiple pages.\n",
        "\n",
        "For privacy/security, here are example targets:\n",
        "- A few major sites‚Äô privacy policies (public pages)\n",
        "- A standards or regulatory page (public pages)\n",
        "- A research lab page\n",
        "\n",
        "**Important research ethics note:** This curriculum is about scraping *public pages responsibly*. We will:\n",
        "- scrape slowly (few pages)\n",
        "- respect Terms of Service / robots.txt when scaling later\n",
        "- avoid personal data collection\n",
        "\n",
        "For Week 1, we‚Äôll do a **small, manual list** of URLs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://www.wikipedia.org/\",\n",
        "    \"https://www.mozilla.org/en-US/privacy/\",\n",
        "    \"https://support.google.com/accounts/answer/112802?hl=en\",  # example help page\n",
        "]\n",
        "\n",
        "dataset = []\n",
        "for u in urls:\n",
        "    try:\n",
        "        dataset.append(analyze_page(u))\n",
        "    except Exception as e:\n",
        "        dataset.append({\"url\": u, \"error\": str(e)})\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Quick descriptive summary (without pandas)\n",
        "\n",
        "Even without pandas, you can compute simple stats.\n",
        "\n",
        "Example:\n",
        "- How many pages mention ‚Äúprivacy‚Äù?\n",
        "- Average number of links?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count mentions\n",
        "privacy_count = sum(1 for row in dataset if row.get(\"mentions_privacy\") is True)\n",
        "cookies_count = sum(1 for row in dataset if row.get(\"mentions_cookies\") is True)\n",
        "\n",
        "# Average links (skip rows that errored)\n",
        "link_counts = [row[\"num_links\"] for row in dataset if \"num_links\" in row]\n",
        "avg_links = sum(link_counts) / len(link_counts) if link_counts else None\n",
        "\n",
        "print(\"Pages mentioning privacy:\", privacy_count, \"/\", len(dataset))\n",
        "print(\"Pages mentioning cookies:\", cookies_count, \"/\", len(dataset))\n",
        "print(\"Average num_links:\", avg_links)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrap-up\n",
        "\n",
        "You just built:\n",
        "- a **basic scraper function**\n",
        "- a **mini dataset**\n",
        "- a **mini descriptive analysis**\n",
        "\n",
        "Next week we‚Äôll:\n",
        "- extract more structured signals (headings, paragraphs)\n",
        "- store results cleanly\n",
        "- start thinking about ‚Äúdata schemas‚Äù for research scraping\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "title": "Week 1 Concepts ‚Äî Python for Privacy & Security Scraping"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
