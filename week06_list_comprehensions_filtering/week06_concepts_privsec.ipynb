{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 06 â€” Text Signals for Human Factors: Readability + Sentiment (light)\n",
        "\n",
        "**Time budget:** ~2 hours  \n",
        "**Goal:** Compute basic text metrics (length, readability approximation) and visualize distributions.\n",
        "\n",
        "**Theme (PhD focus):** Human factors of privacy & security \u2014 scraping public pages (privacy policies, cookie notices, security help pages, standards/regulator guidance) and extracting *UX-relevant* signals.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Responsible scraping note (important)\n",
        "We will only scrape **public pages** and keep the volume small.\n",
        "- Prefer a few pages, not thousands\n",
        "- Respect robots.txt/Terms of Service when you scale later\n",
        "- Avoid collecting personal data\n",
        "- Add delays for politeness when doing multi-page work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Weâ€™ll use `requests` + `BeautifulSoup`. Install if needed:\n",
        "\n",
        "```bash\n",
        "pip install requests beautifulsoup4 pandas matplotlib\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import json\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text metrics that matter for human factors\n",
        "Policies affect comprehension. Simple proxies:\n",
        "- total word count\n",
        "- average sentence length (rough)\n",
        "- readability approximations (very rough)\n",
        "\n",
        "We wonâ€™t overclaim: these are *signals*, not truth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ§  Concept: Text as Data (Tokenization)\n",
        "\n",
        "Computers don't read. They count.\n",
        "\n",
        "To analyze text, we chop it up into pieces called **Tokens**.\n",
        "- **Sentence Tokenization**: Splitting by periods (`.`) or exclamation marks (`!`).\n",
        "- **Word Tokenization**: Splitting by spaces (` `).\n",
        "\n",
        "**Why?**\n",
        "Long sentences are harder to read. By counting words per sentence, we measure *Cognitive Load*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def basic_text_metrics(text: str) -> dict:\n",
        "    words = text.split()\n",
        "    num_words = len(words)\n",
        "    sentences = re.split(r\"[.!?]+\\s+\", text)\n",
        "    sentences = [s for s in sentences if s.strip()]\n",
        "    avg_sentence_len = (num_words / len(sentences)) if sentences else None\n",
        "    return {\"num_words\": num_words, \"num_sentences\": len(sentences), \"avg_sentence_len\": avg_sentence_len}\n",
        "\n",
        "sample = \"We use cookies. You can opt out in settings. We retain data for 30 days.\"\n",
        "basic_text_metrics(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: integrate metrics into a scrape\n",
        "def scrape_text(url: str) -> dict:\n",
        "    r = requests.get(url, timeout=20)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    text = soup.get_text(\" \", strip=True)\n",
        "    row = {\"url\": url, \"status\": r.status_code}\n",
        "    row.update(basic_text_metrics(text))\n",
        "    return row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame([\n",
        "    scrape_text(\"https://www.mozilla.org/en-US/privacy/\"),\n",
        "])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"num_words\"].plot(kind=\"hist\", title=\"Word count\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "title": "Week 06 Concepts â€” Text Signals for Human Factors: Readability + Sentiment (light)"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}