{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 11 \u2014 More Complex Sites: Sitemaps + Multi-source Datasets\n",
        "\n",
        "**Time budget:** ~2 hours  \n",
        "**Goal:** Use sitemap.xml (where available) or curated sources; unify multi-source datasets.\n",
        "\n",
        "**Theme (PhD focus):** Human factors of privacy & security \u2014 scraping public pages (privacy policies, cookie notices, security help pages, standards/regulator guidance) and extracting *UX-relevant* signals.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deliverables\n",
        "- A completed notebook with working code\n",
        "- A dataset variable (`rows` or `df`) saved to disk (CSV/JSON depending on week)\n",
        "- 3\u20135 bullet reflection grounded in human factors/privacy-security research\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Responsible scraping note (important)\n",
        "We will only scrape **public pages** and keep the volume small.\n",
        "- Prefer a few pages, not thousands\n",
        "- Respect robots.txt/Terms of Service when you scale later\n",
        "- Avoid collecting personal data\n",
        "- Add delays for politeness when doing multi-page work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0 \u2014 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import json\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 \u2014 Try sitemap discovery (small sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def try_fetch_sitemap(base_url: str) -> list[str]:\n",
        "    sitemap_url = base_url.rstrip(\"/\") + \"/sitemap.xml\"\n",
        "    r = requests.get(sitemap_url, timeout=20, headers={\"User-Agent\":\"HF-PrivacyScraper/0.1\"})\n",
        "    if r.status_code != 200:\n",
        "        return []\n",
        "    soup = BeautifulSoup(r.text, \"xml\")\n",
        "    return [loc.get_text(strip=True) for loc in soup.find_all(\"loc\")]\n",
        "\n",
        "locs = try_fetch_sitemap(\"https://www.mozilla.org\")\n",
        "print(\"Found:\", len(locs))\n",
        "locs[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 \u2014 Filter sitemap URLs for relevant keywords (privacy/security)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keywords = [\"privacy\", \"security\", \"cookie\", \"data\"]\n",
        "relevant = [u for u in locs if any(k in u.lower() for k in keywords)]\n",
        "relevant[:20], len(relevant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 \u2014 Scrape a small subset (5 max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subset = relevant[:5]\n",
        "rows = []\n",
        "for u in subset:\n",
        "    try:\n",
        "        html = requests.get(u, timeout=20).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = soup.get_text(\" \", strip=True)\n",
        "        rows.append({\"url\": u, \"title\": soup.title.get_text(strip=True) if soup.title else None, **{\n",
        "            \"mentions_choices\": bool(re.search(r\"\\b(opt\\s?-?out|preferences|your choices|controls?)\\b\", text, re.I))\n",
        "        }})\n",
        "    except Exception as e:\n",
        "        rows.append({\"url\": u, \"error\": str(e)})\n",
        "pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- How does sitemap-based sampling differ from \u201cchoose 5 URLs by hand\u201d?\n",
        "- What sampling biases might appear?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "title": "Week 11 Assignment \u2014 More Complex Sites: Sitemaps + Multi-source Datasets"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}