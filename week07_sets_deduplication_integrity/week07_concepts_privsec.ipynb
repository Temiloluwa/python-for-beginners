{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 07 â€” Requests Session + Caching + Reproducibility\n",
        "\n",
        "**Time budget:** ~2 hours  \n",
        "**Goal:** Use sessions, headers, caching strategy, and reproducible runs; introduce pathlib and configs.\n",
        "\n",
        "**Theme (PhD focus):** Human factors of privacy & security \u2014 scraping public pages (privacy policies, cookie notices, security help pages, standards/regulator guidance) and extracting *UX-relevant* signals.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Responsible scraping note (important)\n",
        "We will only scrape **public pages** and keep the volume small.\n",
        "- Prefer a few pages, not thousands\n",
        "- Respect robots.txt/Terms of Service when you scale later\n",
        "- Avoid collecting personal data\n",
        "- Add delays for politeness when doing multi-page work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Weâ€™ll use `requests` + `BeautifulSoup`. Install if needed:\n",
        "\n",
        "```bash\n",
        "pip install requests beautifulsoup4 pandas matplotlib\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import json\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reproducibility upgrades\n",
        "Introduce:\n",
        "- `requests.Session()` for consistent headers\n",
        "- a tiny disk cache (save HTML) to avoid re-downloading\n",
        "- run IDs / timestamps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ§  Concept: The Session (`requests.Session`)\n",
        "\n",
        "Imagine:\n",
        "- `requests.get()` is like opening a **New Incognito Window** for every single click. No memory. No cookies.\n",
        "- `session.get()` is like using your **Main Browser**. It remembers cookies, navigation history, and settings (headers).\n",
        "\n",
        "Use `Session` for speed and consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ§  Concept: Caching (Time Travel)\n",
        "\n",
        "Scraping is slow. The internet changes. Analysis is fast.\n",
        "- **Problem**: If you re-run your code, you re-download everything. 100 pages = 2 minutes.\n",
        "- **Solution**: Save the page to disk (`.html`).\n",
        "- **Logic**: \"If I have the file, read it. If I don't, download it.\"\n",
        "\n",
        "This makes your research **Reproducible**. You can prove what the page looked like on Tuesday."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "CACHE_DIR = Path(\"cache_html\")\n",
        "CACHE_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "session = requests.Session()\n",
        "session.headers.update({\"User-Agent\": \"HF-PrivacyScraper/0.1\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cache_key(url: str) -> str:\n",
        "    safe = re.sub(r\"[^a-zA-Z0-9]+\", \"_\", url)[:120]\n",
        "    return safe + \".html\"\n",
        "\n",
        "def fetch_with_cache(url: str, use_cache: bool = True) -> str:\n",
        "    key = cache_key(url)\n",
        "    path = CACHE_DIR / key\n",
        "    if use_cache and path.exists():\n",
        "        return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    r = session.get(url, timeout=20)\n",
        "    r.raise_for_status()\n",
        "    path.write_text(r.text, encoding=\"utf-8\")\n",
        "    return r.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"https://www.mozilla.org/en-US/privacy/\"\n",
        "html = fetch_with_cache(url, use_cache=True)\n",
        "print(\"Cached bytes:\", len(html))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "title": "Week 07 Concepts â€” Requests Session + Caching + Reproducibility"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}