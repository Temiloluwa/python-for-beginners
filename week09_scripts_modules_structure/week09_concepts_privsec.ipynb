{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 09 \u2014 OOP Scraper Design\n",
        "\n",
        "**Time budget:** ~2 hours  \n",
        "**Goal:** Implement a scraper class with clear responsibilities; introduce dataclasses.\n",
        "\n",
        "**Theme (PhD focus):** Human factors of privacy & security \u2014 scraping public pages (privacy policies, cookie notices, security help pages, standards/regulator guidance) and extracting *UX-relevant* signals.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Responsible scraping note (important)\n",
        "We will only scrape **public pages** and keep the volume small.\n",
        "- Prefer a few pages, not thousands\n",
        "- Respect robots.txt/Terms of Service when you scale later\n",
        "- Avoid collecting personal data\n",
        "- Add delays for politeness when doing multi-page work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Weâ€™ll use `requests` + `BeautifulSoup`. Install if needed:\n",
        "\n",
        "```bash\n",
        "pip install requests beautifulsoup4 pandas matplotlib\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import json\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OOP pattern: a Scraper class\n",
        "A class helps manage:\n",
        "- shared session / headers\n",
        "- caching config\n",
        "- parsing methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ§  Concept: The Blueprint (Class) vs The House (Object)\n",
        "\n",
        "- **Class** (`class PolicyScraper`): The Architectural Drawing. It says \"Every scraper has a session and a fetch method\".\n",
        "- **Object** (`scraper = PolicyScraper()`): The Actual House built from that drawing. You can build 10 different scrapers (houses).\n",
        "\n",
        "### ðŸ§  Concept: `self`\n",
        "- `self` just means **\"My Own\"**.\n",
        "- `self.session` = \"My own session\" (not someone else's).\n",
        "- When a house (Object) wants to open its *own* front door, it uses `self.open_door()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ScrapeResult:\n",
        "    url: str\n",
        "    status: int | None\n",
        "    title: str | None\n",
        "    cues: dict\n",
        "    error: str | None = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyScraper:\n",
        "    def __init__(self, user_agent: str = \"HF-PrivacyScraper/0.1\"):\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\"User-Agent\": user_agent})\n",
        "\n",
        "    def fetch(self, url: str) -> str:\n",
        "        r = self.session.get(url, timeout=20)\n",
        "        r.raise_for_status()\n",
        "        return r.text\n",
        "\n",
        "    def parse(self, url: str, html: str) -> ScrapeResult:\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        text = soup.get_text(\" \", strip=True)\n",
        "        cues = {\n",
        "            \"choices_controls\": bool(re.search(r\"\\b(opt\\s?-?out|preferences|your choices|controls?)\\b\", text, re.I)),\n",
        "            \"retention\": bool(re.search(r\"\\b(retention|retain)\\b\", text, re.I)),\n",
        "            \"third_party\": bool(re.search(r\"\\b(third\\s?-?party|sharing|share)\\b\", text, re.I)),\n",
        "        }\n",
        "        return ScrapeResult(\n",
        "            url=url,\n",
        "            status=200,\n",
        "            title=soup.title.get_text(strip=True) if soup.title else None,\n",
        "            cues=cues,\n",
        "        )\n",
        "\n",
        "    def scrape(self, url: str) -> ScrapeResult:\n",
        "        try:\n",
        "            html = self.fetch(url)\n",
        "            return self.parse(url, html)\n",
        "        except Exception as e:\n",
        "            return ScrapeResult(url=url, status=None, title=None, cues={}, error=str(e))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scraper = PolicyScraper()\n",
        "result = scraper.scrape(\"https://www.mozilla.org/en-US/privacy/\")\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "title": "Week 09 Concepts â€” OOP Scraper Design"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}