{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 03 â€” Pagination + Robustness\n",
        "\n",
        "**Time budget:** ~2 hours  \n",
        "**Goal:** Scrape multiple pages with politeness (rate limiting) and robust error handling; introduce comprehensions.\n",
        "\n",
        "**Theme (PhD focus):** Human factors of privacy & security â€” scraping public pages (privacy policies, cookie notices, security help pages, standards/regulator guidance) and extracting *UX-relevant* signals.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Responsible scraping note (important)\n",
        "We will only scrape **public pages** and keep the volume small.\n",
        "- Prefer a few pages, not thousands\n",
        "- Respect robots.txt/Terms of Service when you scale later\n",
        "- Avoid collecting personal data\n",
        "- Add delays for politeness when doing multi-page work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Weâ€™ll use `requests` + `BeautifulSoup`. Install if needed:\n",
        "\n",
        "```bash\n",
        "pip install requests beautifulsoup4 pandas matplotlib\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import json\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-page scraping patterns\n",
        "We introduce:\n",
        "- Politeness delay (`time.sleep`)\n",
        "- Retry-ish error handling\n",
        "- List comprehensions for clean transforms\n",
        "- Basic logging (print is okay for now)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ§  Concept: Functions (`def`)\n",
        "\n",
        "Think of a Function like a **Kitchen Appliance** (e.g., a Blender).\n",
        "- **Input (Arguments)**: Fruit, Milk (`url`, `timeout`).\n",
        "- **Action (Body)**: Blends them (Executes code).\n",
        "- **Output (Return)**: Smoothie (`response` object).\n",
        "\n",
        "We write functions so we don't have to build the blender every time we want a smoothie. We just use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_get(url: str, timeout: int = 20):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=timeout, headers={\"User-Agent\": \"HF-PrivacyScraper/0.1\"})\n",
        "        r.raise_for_status()\n",
        "        return r\n",
        "    except Exception as e:\n",
        "        return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scrape_many(urls: list[str], delay_s: float = 1.0) -> list[dict]:\n",
        "    rows = []\n",
        "    for u in urls:\n",
        "        result = safe_get(u)\n",
        "        if isinstance(result, Exception):\n",
        "            rows.append({\"url\": u, \"error\": str(result)})\n",
        "        else:\n",
        "            soup = BeautifulSoup(result.text, \"html.parser\")\n",
        "            text = soup.get_text(\" \", strip=True).lower()\n",
        "            rows.append({\n",
        "                \"url\": u,\n",
        "                \"status\": result.status_code,\n",
        "                \"title\": soup.title.get_text(strip=True) if soup.title else None,\n",
        "                \"mentions_choices\": (\"choice\" in text) or (\"opt out\" in text) or (\"preferences\" in text),\n",
        "                \"mentions_retention\": (\"retention\" in text) or (\"retain\" in text),\n",
        "            })\n",
        "        time.sleep(delay_s)\n",
        "    return rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://www.nist.gov/privacy-framework\",\n",
        "    \"https://www.enisa.europa.eu/topics/data-protection\",\n",
        "    \"https://www.mozilla.org/en-US/privacy/\",\n",
        "]\n",
        "rows = scrape_many(urls, delay_s=1.0)\n",
        "rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comprehension example\n",
        "Filter only successful rows:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ§  Concept: List Comprehension\n",
        "\n",
        "It's a \"One-Liner\" loop.\n",
        "\n",
        "**The Long Way:**\n",
        "```python\n",
        "results = []\n",
        "for r in rows:\n",
        "    if \"error\" not in r:\n",
        "        results.append(r)\n",
        "```\n",
        "\n",
        "**The Comprehension Way:**\n",
        "```python\n",
        "results = [r for r in rows if \"error\" not in r]\n",
        "```\n",
        "Read it like English: \"Give me `r` for every `r` in `rows` IF `error` is not in `r`.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ok_rows = [r for r in rows if \"error\" not in r]\n",
        "ok_rows"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "title": "Week 03 Concepts â€” Pagination + Robustness"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}