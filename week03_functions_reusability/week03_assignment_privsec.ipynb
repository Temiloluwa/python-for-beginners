{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 03 â€” Pagination + Robustness\n",
        "\n",
        "**Time budget:** ~2 hours  \n",
        "**Goal:** Scrape multiple pages with politeness (rate limiting) and robust error handling; introduce comprehensions.\n",
        "\n",
        "**Theme (PhD focus):** Human factors of privacy & security \u2014 scraping public pages (privacy policies, cookie notices, security help pages, standards/regulator guidance) and extracting *UX-relevant* signals.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deliverables\n",
        "- A completed notebook with working code\n",
        "- A dataset variable (`rows` or `df`) saved to disk (CSV/JSON depending on week)\n",
        "- 3\u20135 bullet reflection grounded in human factors/privacy-security research\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Responsible scraping note (important)\n",
        "We will only scrape **public pages** and keep the volume small.\n",
        "- Prefer a few pages, not thousands\n",
        "- Respect robots.txt/Terms of Service when you scale later\n",
        "- Avoid collecting personal data\n",
        "- Add delays for politeness when doing multi-page work\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 0 \u2014 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import time\n",
        "import json\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 \u2014 Provide a small list of URLs (8\u201315 max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://www.mozilla.org/en-US/privacy/\",\n",
        "    \"https://www.nist.gov/privacy-framework\",\n",
        "    \"https://www.enisa.europa.eu/topics/data-protection\",\n",
        "    \"https://support.google.com/accounts/answer/6294825?hl=en\",\n",
        "    \"https://www.wikipedia.org/\",\n",
        "    # add a few more\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 \u2014 Implement polite multi-page scrape with delay + error capture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ§  Concept: Rate Limiting (Politeness)\n",
        "\n",
        "Servers aren't infinite. If you request 1,000 pages in 1 second, you look like an attacker (DDoS).\n",
        "- **Golden Rule**: Wait 1-2 seconds between requests.\n",
        "- **Code**: `time.sleep(1.0)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_get(url: str):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=20, headers={\"User-Agent\":\"HF-PrivacyScraper/0.1\"})\n",
        "        r.raise_for_status()\n",
        "        return r\n",
        "    except Exception as e:\n",
        "        return e\n",
        "\n",
        "def scrape_many(urls: list[str], delay_s: float = 1.0) -> list[dict]:\n",
        "    rows = []\n",
        "    for u in urls:\n",
        "        res = safe_get(u)\n",
        "        if isinstance(res, Exception):\n",
        "            rows.append({\"url\": u, \"error\": str(res)})\n",
        "        else:\n",
        "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "            text = soup.get_text(\" \", strip=True).lower()\n",
        "            rows.append({\n",
        "                \"url\": u,\n",
        "                \"status\": res.status_code,\n",
        "                \"title\": soup.title.get_text(strip=True) if soup.title else None,\n",
        "                \"mentions_choices\": (\"opt out\" in text) or (\"preferences\" in text) or (\"your choices\" in text),\n",
        "                \"mentions_retention\": (\"retention\" in text) or (\"retain\" in text),\n",
        "            })\n",
        "        time.sleep(delay_s)\n",
        "    return rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = scrape_many(urls, delay_s=1.0)\n",
        "rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 \u2014 Use comprehensions to filter/transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ok_rows = [r for r in rows if \"error\" not in r]\n",
        "ok_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- What kinds of failures occurred (timeouts, 403s, etc.)?\n",
        "- How would failures bias a research dataset?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "title": "Week 03 Assignment â€” Pagination + Robustness"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}